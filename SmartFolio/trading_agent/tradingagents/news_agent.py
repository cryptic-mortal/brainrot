from __future__ import annotations

import contextlib
import html
import re
from dataclasses import dataclass
from datetime import date, datetime, timedelta, timezone
from email.utils import parsedate_to_datetime
from typing import Any, Dict, Iterable, List, Optional, Tuple
from urllib.error import URLError
from urllib.parse import quote_plus
from urllib.request import urlopen

import xml.etree.ElementTree as ET

import pathway as pw
import yfinance as yf

from tradingagents import llm_client

try:
    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
except ImportError:  # pragma: no cover - handled at runtime
    SentimentIntensityAnalyzer = None  # type: ignore

_SENTIMENT_ANALYSER = SentimentIntensityAnalyzer() if SentimentIntensityAnalyzer else None

@dataclass
class NewsArticle:
    headline: str
    published_at: Optional[str]
    summary: Optional[str]
    source: Optional[str]
    url: Optional[str]
    sentiment: str
    sentiment_score: int


@dataclass
class NewsWeightReport:
    ticker: str
    weight: float
    as_of: str
    lookback_days: int
    judgement: str
    points: List[str]
    articles: List[NewsArticle]
    generated_via_llm: bool = False

    def to_markdown(self, include_articles: bool = True) -> str:
        header = (
            f"# News-Based Weight Review: {self.ticker}\n\n"
            f"- **As of:** {self.as_of}\n"
            f"- **Assigned Weight:** {self.weight:.2%}\n"
            f"- **News Lookback:** {self.lookback_days} day(s)\n\n"
        )

        bullet_lines = "\n".join(f"- {point}" for point in self.points)
        sections = [header, "## Coverage Assessment\n", bullet_lines, "\n"]

        if include_articles and self.articles:
            sections.extend(["## Notable Headlines\n", _format_articles_table(self.articles), "\n"])

        return "".join(sections)


def _parse_as_of(as_of: str) -> date:
    try:
        return datetime.strptime(as_of, "%Y-%m-%d").date()
    except ValueError as exc:
        raise ValueError("as_of must be in YYYY-MM-DD format") from exc


def _article_to_dict(article: NewsArticle) -> Dict[str, Any]:
    return {
        "headline": article.headline,
        "published_at": article.published_at,
        "summary": article.summary,
        "source": article.source,
        "url": article.url,
        "sentiment": article.sentiment,
        "sentiment_score": int(article.sentiment_score),
    }


def _article_from_dict(payload: Dict[str, Any]) -> NewsArticle:
    sentiment_score = payload.get("sentiment_score", 0)
    try:
        sentiment_int = int(sentiment_score)
    except (TypeError, ValueError):
        sentiment_int = 0
    return NewsArticle(
        headline=str(payload.get("headline", "")),
        published_at=payload.get("published_at"),
        summary=payload.get("summary"),
        source=payload.get("source"),
        url=payload.get("url"),
        sentiment=str(payload.get("sentiment", "neutral")),
        sentiment_score=sentiment_int,
    )


def _compute_news_payload(
    ticker: str,
    weight: float,
    as_of: str,
    lookback_days: int,
    max_articles: int,
    use_llm: bool,
    llm_model: Optional[str],
) -> Dict[str, Any]:
    as_of_date = _parse_as_of(as_of)
    lookback = int(lookback_days)
    if lookback <= 0:
        raise ValueError("lookback_days must be positive")
    max_count = int(max_articles)
    if max_count <= 0:
        raise ValueError("max_articles must be positive")

    start_date = as_of_date - timedelta(days=lookback)

    articles = _fetch_news(ticker, start_date, as_of_date)
    # Debug: log fetched article counts before and after scoring
    try:
        print(f"[DEBUG news_agent] {ticker}: fetched_raw_articles={len(articles)}")
    except Exception:
        pass
    articles = _score_articles(articles)
    try:
        print(f"[DEBUG news_agent] {ticker}: scored_articles={len(articles)}, use_llm={use_llm}, llm_model={llm_model}")
    except Exception:
        pass
    articles = articles[:max_count]

    judgement, supporting_points = _build_opinion(weight, articles)
    points = [judgement] + supporting_points
    points = [str(point) for point in points[:4]]
    generated_via_llm = False

    if use_llm:
        article_summaries = _articles_prompt_digest(articles)
        net_sentiment = sum(article.sentiment_score for article in articles)
        llm_points = llm_client.summarise_news(
            ticker=ticker,
            weight=weight,
            as_of=as_of_date.isoformat(),
            lookback_days=lookback,
            article_summaries=article_summaries,
            net_sentiment=net_sentiment,
            max_points=4,
            model=llm_model,
        )
        if llm_points:
            cleaned_points = [str(point) for point in llm_points if point]
            points = cleaned_points[:4] or points
            if points:
                judgement = points[0]
            generated_via_llm = True

    return {
        "ticker": ticker,
        "weight": float(weight),
        "as_of": as_of_date.isoformat(),
        "lookback_days": lookback,
        "judgement": judgement,
        "points": points,
        "articles": [_article_to_dict(article) for article in articles],
        "generated_via_llm": generated_via_llm,
    }


def _run_news_pipeline(
    *,
    ticker: str,
    weight: float,
    as_of: str,
    lookback_days: int,
    max_articles: int,
    use_llm: bool,
    llm_model: Optional[str],
) -> Dict[str, Any]:
    payload = _compute_news_payload(
        ticker,
        weight,
        as_of,
        lookback_days,
        max_articles,
        use_llm,
        llm_model,
    )
    payload_table = pw.debug.table_from_rows(
        pw.schema_from_types(payload=pw.Json),
        [(pw.Json(payload),)],
    )
    keys, columns = pw.debug.table_to_dicts(payload_table)
    if not keys:
        raise RuntimeError("News analysis produced no output.")
    payload = columns["payload"][keys[0]]
    if isinstance(payload, pw.Json):
        return payload.value
    return payload


def _payload_to_news_report(payload: Dict[str, Any]) -> NewsWeightReport:
    articles_raw = payload.get("articles", [])
    articles: List[NewsArticle] = []
    for entry in articles_raw:
        if isinstance(entry, NewsArticle):
            articles.append(entry)
            continue
        if not isinstance(entry, dict):
            continue
        with contextlib.suppress(Exception):
            articles.append(_article_from_dict(entry))

    points_raw = payload.get("points", [])
    points = [str(point) for point in points_raw if point is not None]

    return NewsWeightReport(
        ticker=str(payload.get("ticker", "")),
        weight=float(payload.get("weight", 0.0)),
        as_of=str(payload.get("as_of", "")),
        lookback_days=int(payload.get("lookback_days", 0)),
        judgement=str(payload.get("judgement", "")),
        points=points,
        articles=articles,
        generated_via_llm=bool(payload.get("generated_via_llm", False)),
    )


class NewsWeightReviewAgent:
    """Reviews an assigned portfolio weight against recent news flow using Pathway."""

    def __init__(self, *, default_as_of: Optional[date] = None):
        self._default_as_of = default_as_of or date.today()

    def generate_report(
        self,
        ticker: str,
        weight: float,
        *,
        as_of: Optional[str] = None,
        lookback_days: int = 7,
        max_articles: int = 8,
        use_llm: bool = False,
        llm_model: Optional[str] = None,
    ) -> NewsWeightReport:
        clean_ticker = ticker.strip().upper()
        if not clean_ticker:
            raise ValueError("Ticker symbol cannot be empty")
        if not (0.0 <= weight <= 1.0):
            raise ValueError("Weight must be between 0.0 and 1.0 inclusive")
        if lookback_days <= 0:
            raise ValueError("Lookback window must be positive")
        if max_articles <= 0:
            raise ValueError("max_articles must be positive")

        resolved_as_of = self._default_as_of if not as_of else _parse_as_of(as_of)
        as_of_str = resolved_as_of.isoformat()

        payload = _run_news_pipeline(
            ticker=clean_ticker,
            weight=float(weight),
            as_of=as_of_str,
            lookback_days=int(lookback_days),
            max_articles=int(max_articles),
            use_llm=bool(use_llm),
            llm_model=llm_model,
        )
        return _payload_to_news_report(payload)


def _fetch_news(ticker: str, start_date: date, end_date: date) -> List[NewsArticle]:
    primary = _fetch_google_news(ticker, start_date, end_date)
    if primary:
        return primary
    return _fetch_yfinance_news(ticker, start_date, end_date)


def _fetch_google_news(ticker: str, start_date: date, end_date: date) -> List[NewsArticle]:
    query = quote_plus(f"{ticker} stock")
    url = (
        "https://news.google.com/rss/search?q="
        f"{query}&hl=en-US&gl=US&ceid=US:en"
    )

    try:
        with contextlib.closing(urlopen(url, timeout=10)) as response:
            payload = response.read()
    except URLError as e:
        print(f"[DEBUG news_agent] _fetch_google_news URLError for {ticker}: {e}")
        return []
    except TimeoutError as e:
        print(f"[DEBUG news_agent] _fetch_google_news TimeoutError for {ticker}: {e}")
        return []

    try:
        root = ET.fromstring(payload)
    except ET.ParseError:
        return []

    articles: List[NewsArticle] = []
    for item in root.findall(".//item"):
        title = (item.findtext("title") or "").strip()
        if not title:
            continue

        pub_date_raw = item.findtext("pubDate")
        publish_dt: Optional[datetime] = None
        if pub_date_raw:
            try:
                publish_dt = parsedate_to_datetime(pub_date_raw)
                if publish_dt.tzinfo is None:
                    publish_dt = publish_dt.replace(tzinfo=timezone.utc)
                else:
                    publish_dt = publish_dt.astimezone(timezone.utc)
            except (TypeError, ValueError):
                publish_dt = None
        if publish_dt is None:
            continue
        if publish_dt.date() < start_date or publish_dt.date() > end_date:
            continue

        summary_raw = item.findtext("description") or ""
        summary = _strip_html(summary_raw).strip() or None
        source_elem = item.find("{http://news.google.com/newssources}news-source")
        source = source_elem.text.strip() if source_elem is not None and source_elem.text else None
        link = (item.findtext("link") or "").strip() or None

        articles.append(
            NewsArticle(
                headline=html.unescape(title),
                published_at=publish_dt.isoformat(),
                summary=html.unescape(summary) if summary else None,
                source=source,
                url=link,
                sentiment="neutral",
                sentiment_score=0,
            )
        )

    articles.sort(key=lambda article: article.published_at or "", reverse=True)
    deduped = _deduplicate_articles(articles)
    try:
        print(f"[DEBUG news_agent] _fetch_google_news {ticker}: parsed={len(articles)}, deduped={len(deduped)}")
    except Exception:
        pass
    return deduped


def _fetch_yfinance_news(ticker: str, start_date: date, end_date: date) -> List[NewsArticle]:
    try:
        payload = yf.Ticker(ticker).news or []
    except Exception:
        payload = []

    articles: List[NewsArticle] = []
    for item in payload:
        if not isinstance(item, dict):
            continue
        published = _extract_publish_datetime(item)
        if published is None:
            continue
        if published.date() < start_date or published.date() > end_date:
            continue
        headline = (item.get("title") or item.get("headline") or "").strip()
        if not headline:
            continue
        summary = (item.get("summary") or item.get("content") or "").strip() or None
        source = (item.get("publisher") or item.get("source") or "").strip() or None
        url = (item.get("link") or item.get("url") or "").strip() or None
        articles.append(
            NewsArticle(
                headline=headline,
                published_at=published.isoformat(),
                summary=summary,
                source=source,
                url=url,
                sentiment="neutral",
                sentiment_score=0,
            )
        )

    articles.sort(key=lambda article: article.published_at or "", reverse=True)
    deduped = _deduplicate_articles(articles)
    try:
        print(f"[DEBUG news_agent] _fetch_yfinance_news {ticker}: parsed={len(articles)}, deduped={len(deduped)}")
    except Exception:
        pass
    return deduped


def _score_articles(articles: List[NewsArticle]) -> List[NewsArticle]:
    scored: List[NewsArticle] = []
    for article in articles:
        text = " ".join(filter(None, [article.headline, article.summary]))
        label, score = _score_text(text)
        scored.append(
            NewsArticle(
                headline=article.headline,
                published_at=article.published_at,
                summary=article.summary,
                source=article.source,
                url=article.url,
                sentiment=label,
                sentiment_score=score,
            )
        )
    scored.sort(key=lambda a: (a.sentiment_score, a.headline.lower()), reverse=True)
    return scored


def _build_opinion(weight: float, articles: List[NewsArticle]) -> Tuple[str, List[str]]:
    if not articles:
        judgement = (
            "No recent vendor news was available; maintain the current allocation until coverage improves."
        )
        return judgement, ["Absence of fresh headlines keeps the allocation decision data-light."]

    positives = [a for a in articles if a.sentiment_score > 0]
    negatives = [a for a in articles if a.sentiment_score < 0]
    net_score = sum(a.sentiment_score for a in articles)

    judgement = _compose_weight_statement(weight, net_score, len(positives), len(negatives))

    supporting: List[str] = []
    coverage_summary = _coverage_summary(len(positives), len(negatives), len(articles))
    supporting.append(coverage_summary)

    for article in _top_articles(positives, negatives):
        tone = "positive" if article.sentiment_score > 0 else "negative"
        source = article.source or "vendor"
        date_str = article.published_at or "recent"
        supporting.append(
            f"{tone.title()} headline from {source} ({date_str}): {article.headline}"
        )
        if len(supporting) >= 3:
            break

    if len(supporting) < 3 and not negatives and positives:
        supporting.append("Coverage skewed positive during the window with no negative headlines captured.")
    if len(supporting) < 3 and not positives and negatives:
        supporting.append("Coverage skewed negative during the window with no offsetting positive headlines.")

    return judgement, supporting


def _top_articles(positives: List[NewsArticle], negatives: List[NewsArticle]) -> Iterable[NewsArticle]:
    ordered = sorted(positives, key=lambda a: -a.sentiment_score) + sorted(
        negatives, key=lambda a: a.sentiment_score
    )
    return ordered


def _coverage_summary(pos_count: int, neg_count: int, total: int) -> str:
    if total == 0:
        return "Coverage volume was negligible over the review window."
    neutral_count = total - pos_count - neg_count
    return (
        f"News tone snapshot: {pos_count} positive, {neg_count} negative, {neutral_count} neutral items in the sample."
    )


def _articles_prompt_digest(articles: List[NewsArticle]) -> str:
    lines: List[str] = []
    for article in articles:
        tone = article.sentiment
        date_str = article.published_at or "recent"
        source = article.source or "vendor"
        summary = article.summary or "(no summary provided)"
        lines.append(
            f"- {tone.title()} | {source} | {date_str}: {article.headline} â€” {summary}"
        )
    return "\n".join(lines)


def _compose_weight_statement(
    weight: float, net_score: int, pos_count: int, neg_count: int
) -> str:
    weight_pct = weight * 100.0
    return (
        f"Headline sentiment score: {net_score} ({pos_count} positive / {neg_count} negative) alongside a {weight_pct:.1f}% allocation."
    )


def _score_text(text: str) -> Tuple[str, int]:
    cleaned = text.strip()
    if not cleaned:
        return "neutral", 0

    if _SENTIMENT_ANALYSER is None:
        return "neutral", 0

    compound = _SENTIMENT_ANALYSER.polarity_scores(cleaned)["compound"]
    if compound >= 0.1:
        return "positive", 1
    if compound <= -0.1:
        return "negative", -1
    return "neutral", 0


def _extract_publish_datetime(item: dict) -> Optional[datetime]:
    raw = item.get("providerPublishTime")
    if raw is not None:
        try:
            return datetime.fromtimestamp(int(raw), tz=timezone.utc)
        except (OSError, OverflowError, TypeError, ValueError):
            pass

    for key in ("pubDate", "publishedAt", "date", "time"):
        raw_value = item.get(key)
        if raw_value is None:
            continue
        if isinstance(raw_value, (int, float)):
            try:
                return datetime.fromtimestamp(float(raw_value), tz=timezone.utc)
            except (OSError, OverflowError, ValueError):
                continue
        if isinstance(raw_value, str):
            candidate = raw_value.strip()
            if not candidate:
                continue
            if candidate.isdigit():
                try:
                    return datetime.fromtimestamp(int(candidate), tz=timezone.utc)
                except (OSError, OverflowError, ValueError):
                    continue
            try:
                return datetime.fromisoformat(candidate.replace("Z", "+00:00"))
            except ValueError:
                continue
    return None


def _strip_html(value: str) -> str:
    return re.sub(r"<[^>]+>", "", value)


def _deduplicate_articles(articles: List[NewsArticle]) -> List[NewsArticle]:
    seen: set[str] = set()
    deduped: List[NewsArticle] = []
    for article in articles:
        key = article.headline.lower().strip()
        if not key or key in seen:
            continue
        seen.add(key)
        deduped.append(article)
    return deduped


def _format_articles_table(articles: List[NewsArticle]) -> str:
    header = "| Date | Source | Tone | Headline |\n| --- | --- | --- | --- |"
    rows = []
    for article in articles:
        date_str = article.published_at.split("T")[0] if article.published_at else "--"
        source = article.source or "--"
        tone = article.sentiment
        headline = article.headline.replace("|", "/")
        rows.append(f"| {date_str} | {source} | {tone} | {headline} |")
    return "\n".join([header] + rows)
